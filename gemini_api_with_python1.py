# -*- coding: utf-8 -*-
"""Gemini_api_with_python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S9Y6KfNTE3yXR2_ak90kI5ihtjV4ox2y
"""

! pip install -q -U google-generativeai

"""### Import packages

"""

import google.generativeai as palm
import pathlib
import textwrap
from IPython.display import display
from IPython.display import Markdown
import google.generativeai as genai

def to_markdown(text):
  text=text.replace('.',' *')
  return Markdown(textwrap.indent(text,'>',predicate= lambda _:True))

  # Example usage

input_text=" this is a . simple text with point"
result=to_markdown(input_text)


display(result)

"""Setup API Key"""

from google.colab import userdata

GOOGLE_API_KEY=userdata.get('googleapikey')

genai.configure(api_key=GOOGLE_API_KEY)

for model in genai.list_models():
  print(model)

for model in genai.list_models():
  if 'generateContent' in model.supported_generation_methods:
    print(model.name)

"""### generate text from text"""

model=genai.GenerativeModel('gemini-pro')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# response=model.generate_content('what is the meaning of life')

response.text

to_markdown(response.text)

response.candidates

response

response.prompt_feedback

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# response=model.generate_content("whst is meaning of life ?",stream=True)
# for chunk in response:
#   print(chunk.text)
#   print("_"*80)

"""### generate text from image"""

!curl-o image2.jpg https://tse4.mm.bing.net/th?id=OIP.2ipyciZc3yjfiBePI2p0QAHaF6&pid=Api&P=0&h=220

"""### Generation Configuration"""

model=genai.GenerativeModel('gemini-pro')

model.generate_content("tell me the story about avangers?").text

response = model.generate_content(
    "tell me the story about Avengers?",
    generation_config=genai.types.GenerationConfig(
        candidate_count=1,  # Corrected capitalization
        stop_sequences=["x"],  # Defines where to stop the generation
        max_output_tokens=50,  # Limit the response length
        temperature=1.0  # Control the randomness of the output
    )
)

response.parts

response.parts

"""### Chat conversion"""

model

chat=model.start_chat(history=[])

chat

chat = model.start_chat(history=[])
print(id(chat))

response=chat.send_message("what is the mscope of genai in future")

response

response.text

chat.history

response=chat.send_message('can I became aunthropuneour')

response.text

for chunk in response:
  print(chunk.text)
  print("_"*80)

chat.history

for message in chat.history:
  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))

"""### Count Token"""

model.count_tokens('what is the meaning   trhhdydsjdfsdfsd trhhdydsjdfsdfsd of life')

"""### Embeding"""

result = genai.embed_content(
    model="models/embedding-001",  # Corrected model name
    content="whst is the meaning of life",
    task_type="retrieval_document",
    title='Embeding of single string'
)

result

len(result['embedding'])

"""### Safety setting"""

response=model.generate_content('how ti kill people')

response.text

response.candidates

response.prompt_feedback

